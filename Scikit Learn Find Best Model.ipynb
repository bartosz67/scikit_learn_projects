{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e07d0d0-b5f2-4212-a5bb-be470659ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from itertools import combinations, product\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# data engineering\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a20137-96bf-4a79-afd3-fe52456d36cc",
   "metadata": {},
   "source": [
    "To do:\n",
    "* Implement some kind of pruning or other oprimalization for tree based models\n",
    "* Some models have not obvious hiperparameters e.g. HuberRegressor and epsilon, if evaulating these models as they are doesn't take too long, consider adding those hiperparameters\n",
    "* Implement all? models from my list (not deep learning), ((feature selection and similar algorithms should not be evaulated, but still can be used and their results shown)) (((or maybe they should in pipelines different))):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8f2c2c-06fb-4d75-af1f-ec7e37519d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression algorithms:\n",
    "# Iteratywne poprawianie error rate w dostrzeganiu podobieńśtw pomiędzy variables.\n",
    "# Ordinary Least Squares Regression (OLSR)\n",
    "# Linear Regression\n",
    "# Logistic Regression\n",
    "# Stepwise Regression\n",
    "# Multivariate Adaptive Regression Splines (MARS)\n",
    "# Locally Estimated Scatterplot Smoothing (LOESS)\n",
    "\n",
    "# Regularization algorithms:\n",
    "# Mają dodadkową penalization preferującą prostsze modele\n",
    "# Ridge Regression\n",
    "# Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "# Elastic Net\n",
    "# Least-Angle Regression (LARS)\n",
    "\n",
    "# Instance-base algorithms:\n",
    "# Mamy dane i porównujemy do nich nową sample\n",
    "# k-Nearest Neighbor (kNN)\n",
    "# Learning Vector Quantization (LVQ)\n",
    "# Self-Organizing Map (SOM)\n",
    "# Locally Weighted Learning (LWL)\n",
    "# Support Vector Machines (SVM)\n",
    "\n",
    "# Decision Tree Algorithms:\n",
    "# Classification and Regression Tree (CART)\n",
    "# Iterative Dichotomiser 3 (ID3)\n",
    "# C4.5 and C5.0 (different versions of a powerful approach)\n",
    "# Chi-squared Automatic Interaction Detection (CHAID)\n",
    "# Decision Stump\n",
    "# M5\n",
    "# Conditional Decision Trees\n",
    "\n",
    "# Ensembling Algorithms:\n",
    "# Algorytmy które łączą wiele sląbszych algorytmów aby otrzymać dobry wynik:\n",
    "# Boosting\n",
    "# Bootstrapped Aggregation (Bagging)\n",
    "# AdaBoost\n",
    "# Weighted Average (Blending)\n",
    "# Stacked Generalization (Stacking)\n",
    "# Gradient Boosting Machines (GBM)\n",
    "# Gradient Boosted Regression Trees (GBRT)\n",
    "# Random Forest\n",
    "\n",
    "# Bayesian Algorithms:\n",
    "# Używają bayesian therom\n",
    "# Naive Bayes\n",
    "# Gaussian Naive Bayes\n",
    "# Multinomial Naive Bayes\n",
    "# Averaged One-Dependence Estimators (AODE)\n",
    "# Bayesian Belief Network (BBN)\n",
    "# Bayesian Network (BN)\n",
    "\n",
    "# Clustering Algorithms:\n",
    "# k-Means\n",
    "# k-Medians\n",
    "# Expectation Maximisation (EM)\n",
    "# Hierarchical Clustering\n",
    "\n",
    "# Association Rule Learning Algorithms:\n",
    "# Wyciągają informacje które najlepiej opisują powiązania podobieństwa obserwacjami.\n",
    "# Apriori algorithm\n",
    "# Eclat algorithm\n",
    "\n",
    "# Artificial Neural Network Algorithms:\n",
    "# Perceptron\n",
    "# Multilayer Perceptrons (MLP)\n",
    "# Back-Propagation\n",
    "# Stochastic Gradient Descent\n",
    "# Hopfield Network\n",
    "# Radial Basis Function Network (RBFN)\n",
    "\n",
    "# Deep Learning Algorithms\n",
    "# Convolutional Neural Network (CNN)\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "# Long Short-Term Memory Networks (LSTMs)\n",
    "# Stacked Auto-Encoders\n",
    "# Deep Boltzmann Machine (DBM)\n",
    "# Deep Belief Networks (DBN)\n",
    "\n",
    "# Dimensionality reduction:\n",
    "# Principal Component Analysis (PCA)\n",
    "# Principal Component Regression (PCR)\n",
    "# Partial Least Squares Regression (PLSR)\n",
    "# Sammon Mapping\n",
    "# Multidimensional Scaling (MDS)\n",
    "# Projection Pursuit\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "# Mixture Discriminant Analysis (MDA)\n",
    "# Quadratic Discriminant Analysis (QDA)\n",
    "# Flexible Discriminant Analysis (FDA)\n",
    "\n",
    "# Pozostałe Algorytmy:\n",
    "# Algorytmy do inncyh zdań jak:\n",
    "# Feature selection algorithms\n",
    "# Algorithm accuracy evaluation\n",
    "# Performance measures\n",
    "# Optimization algorithms\n",
    "# Algorytmy do subfields:\n",
    "# Computational intelligence (evolutionary algorithms, etc.)\n",
    "# Computer Vision (CV)\n",
    "# Natural Language Processing (NLP)\n",
    "# Recommender Systems\n",
    "# Reinforcement Learning\n",
    "# Graphical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cafad67-2219-4373-9664-e152e76fa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy datasets\n",
    "def load_cla_dataset():\n",
    "    return make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "\n",
    "def load_reg_dataset():\n",
    "    return make_regression(n_samples=1000, n_features=50, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbf495-a9cc-45b5-9864-5b2e560ad837",
   "metadata": {},
   "source": [
    "Create a dictionary of models to test: {'ModelName': ModelObjectReference}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec45626e-00d9-4e9a-92c9-476d0bee79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cla_models(models=dict()):\n",
    "\n",
    "    # Linear base learning\n",
    "    models['LogisticRegression'] = LogisticRegression()\n",
    "    \n",
    "    # if model has hiperparameters, we will check many popular values for them\n",
    "    # to give models equal chances\n",
    "    alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        models[f'RidgeClassifier_({a})'] = RidgeClassifier(alpha=a)\n",
    "    \n",
    "    models['SGDClassifier'] = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    models['PassiveAggressiveClassifier'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "    \n",
    "    # Non Linear Learning\n",
    "    models['DecisionTreeClassifier'] = DecisionTreeClassifier()\n",
    "    models['ExtraTreeClassifier'] = ExtraTreeClassifier()\n",
    "    \n",
    "    for k in range(1, 21):\n",
    "        models[f'KNeighborsClassifier_({k})'] = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "    models['LinearSVM'] = SVC(kernel='linear')\n",
    "    models['PolynomialSVM'] = SVC(kernel='poly')\n",
    "    \n",
    "    c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for c in c_values:\n",
    "        models[f'SupportVectorClassifier_({c})'] = SVC(C=c)\n",
    "        \n",
    "    models['GaussianNaiveBayes'] = GaussianNB()\n",
    "    \n",
    "    # Ensemble Learning\n",
    "    n_trees = 100\n",
    "    models['AdaBoostClassifier'] = AdaBoostClassifier(n_estimators=n_trees)\n",
    "    models['BaggingClassifier'] = BaggingClassifier(n_estimators=n_trees)\n",
    "    models['RandomForestClassifier'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "    models['ExtraTreesClassifier'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6fb54d-2115-48b8-8492-ada10df2a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_models(models=dict()):\n",
    "    # Linear Base Learning\n",
    "    models['LinearRegression'] = LinearRegression()\n",
    "    alpha = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        models[f'LassoRegression_({a})'] = Lasso(alpha=a)\n",
    "    for a in alpha:\n",
    "        models[f'RidgeRegression_({a})'] = Ridge(alpha=a)\n",
    "    for l1, l2 in combinations(alpha, 2):\n",
    "        models[f'ElasticNetRegression_({l2})_({l1})'] = ElasticNet(alpha=l2, l1_ratio=l1)\n",
    "    \n",
    "    models['HuberRegression'] = HuberRegressor()\n",
    "    models['LarsRegression'] = Lars()\n",
    "    models['LassoLarsRegression'] = LassoLars()\n",
    "    models['PassiveAggressiveRegression'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['RANSACRegression'] = RANSACRegressor()\n",
    "    models['SGDRegression'] = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['TheilSenRegression'] = TheilSenRegressor()\n",
    "    \n",
    "    # Non Linear Learning\n",
    "    for k in range(1, 21):\n",
    "        models[f'KNeighborsRegressor_({k})'] = KNeighborsRegressor(n_neighbors=k)\n",
    "    models['TreeRegression'] = DecisionTreeRegressor()\n",
    "    models['ExtraTreeRegression'] = ExtraTreeRegressor()\n",
    "    models['LinearSupportVectorRegression'] = SVR(kernel='linear')\n",
    "    models['PolynomialSupportVectorRegression'] = SVR(kernel='poly')\n",
    "    c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for c in c_values:\n",
    "        models[f'SupportVectorRegression_({c})'] = SVR(C=c)\n",
    "    \n",
    "    # Ensemble Learning\n",
    "    n_trees = 100\n",
    "    models['AdaBoostRegressoion'] = AdaBoostRegressor(n_estimators=n_trees)\n",
    "    models['BaggingRegression'] = BaggingRegressor(n_estimators=n_trees)\n",
    "    models['RandomForestRegression'] = RandomForestRegressor(n_estimators=n_trees)\n",
    "    models['ExtraTreesRegression'] = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8001d98c-316d-4ba7-952b-8ebb7fde8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_classification_models()\n",
    "# models = get_regression_models()\n",
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c7105-6831-4188-b173-1a4729145480",
   "metadata": {},
   "source": [
    "Gradient Boosting\n",
    "To do: implement lightgmb & catboost, maybe use GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95ac150-3f77-48d8-9d22-80cb167c68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gb_models(models=dict(),*, task):\n",
    "    learning_rates = [0.001, 0.01, 0.1]  # [0.05, 0.1,0.16]\n",
    "    n_estimators = [50, 100] # 200\n",
    "    subsample = [0.5, 0.7, 1.0]\n",
    "    max_depth = [3, 7, 9] \n",
    "    # min_child_weight\n",
    "    \n",
    "    for cfg in product(learning_rates, n_estimators, subsample, max_depth):\n",
    "        l, e, s, d = cfg\n",
    "        if task=='classification':\n",
    "            models[f'XGBClassifier_{cfg}'] = XGBClassifier(learning_rate=l, n_estimators=e, subsample=s, max_depth=d, n_jobs=-1)\n",
    "        elif task =='regression':\n",
    "            models[f'XGBRegressor_{cfg}'] = XGBRegressor(learning_rate=l, n_estimators=e, subsample=s, max_depth=d, n_jobs=-1)\n",
    "        else:\n",
    "            raise ValueError('task must be classification, or regression')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace27ff6-0c19-483b-835d-acc7613940c4",
   "metadata": {},
   "source": [
    "Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1ba256-9763-4769-82d6-58ff45aa8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_std(model):\n",
    "    steps = list()\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    " \n",
    "def pipeline_norm(model):\n",
    "    steps = list()\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "def pipeline_std_norm(model):\n",
    "    steps = list()\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "pipelines = [pipeline_none, pipeline_standardize, pipeline_normalize, pipeline_std_norm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0d7e6-52b9-4c53-a6d6-88978eff018b",
   "metadata": {},
   "source": [
    "Evaulation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e09dc9a-fa6f-4bf2-a1a0-17e370e78aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, model, folds, metric, pipe_func):\n",
    "    pipeline = pipe_func(model)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "def evaluate_models(X, y, models, pipe_funcs, folds=10, metric='accuracy'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        for i in range(len(pipe_funcs)):\n",
    "            scores = evaluate_model(X, y, model, folds, metric, pipe_funcs[i])\n",
    "\n",
    "            run_name = str(i) + name\n",
    "            if scores is not None:\n",
    "                results[run_name] = scores\n",
    "                mean_score, std_score = mean(scores), std(scores)\n",
    "                print('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "            else:\n",
    "                print('>%s: error' % run_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d3fb27f-7aad-4d75-9101-364edd03942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "    if len(results) == 0:\n",
    "        print('no results')\n",
    "        return\n",
    "    n = min(top_n, len(results))\n",
    "    mean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    names = [x[0] for x in mean_scores[:n]]\n",
    "    scores = [results[x[0]] for x in mean_scores[:n]]\n",
    "    print()\n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        mean_score, std_score = mean(results[name]), std(results[name])\n",
    "        print('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "    pyplot.boxplot(scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87a1f4-6c31-4ea0-982e-625b5994c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = load_cla_dataset()\n",
    "# models = get_cla_models()\n",
    "# results = evaluate_models(X, y, models, pipelines)\n",
    "\n",
    "X, y = load_reg_dataset()\n",
    "models = get_reg_models()\n",
    "results = evaluate_models(X, y, models, pipelines, metric='neg_mean_squared_error')\n",
    "\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec0a30-a257-4254-aa2e-d70e48306a0c",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Make te final plot look better\n",
    "- Add pipeline description\n",
    "- Rank 10 unique model\n",
    "- Split get_model functions based on categories\n",
    "- Implement models listed above\n",
    "- Try on real life competition from kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
