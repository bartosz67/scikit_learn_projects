{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e07d0d0-b5f2-4212-a5bb-be470659ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from itertools import combinations\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# data engineering\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a20137-96bf-4a79-afd3-fe52456d36cc",
   "metadata": {},
   "source": [
    "To do:\n",
    "* Implement some kind of pruning or other oprimalization for tree based models\n",
    "* Some models have not obvious hiperparameters e.g. HuberRegressor and epsilon, if evaulating these models as they are doesn't take too long, consider adding those hiperparameters\n",
    "* Implement all? models from my list (not deep learning), ((feature selection and similar algorithms should not be evaulated, but still can be used and their results shown)) (((or maybe they should in pipelines different))):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b8f2c2c-06fb-4d75-af1f-ec7e37519d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression algorithms:\n",
    "# Iteratywne poprawianie error rate w dostrzeganiu podobieńśtw pomiędzy variables.\n",
    "# Ordinary Least Squares Regression (OLSR)\n",
    "# Linear Regression\n",
    "# Logistic Regression\n",
    "# Stepwise Regression\n",
    "# Multivariate Adaptive Regression Splines (MARS)\n",
    "# Locally Estimated Scatterplot Smoothing (LOESS)\n",
    "\n",
    "# Regularization algorithms:\n",
    "# Mają dodadkową penalization preferującą prostsze modele\n",
    "# Ridge Regression\n",
    "# Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "# Elastic Net\n",
    "# Least-Angle Regression (LARS)\n",
    "\n",
    "# Instance-base algorithms:\n",
    "# Mamy dane i porównujemy do nich nową sample\n",
    "# k-Nearest Neighbor (kNN)\n",
    "# Learning Vector Quantization (LVQ)\n",
    "# Self-Organizing Map (SOM)\n",
    "# Locally Weighted Learning (LWL)\n",
    "# Support Vector Machines (SVM)\n",
    "\n",
    "# Decision Tree Algorithms:\n",
    "# Classification and Regression Tree (CART)\n",
    "# Iterative Dichotomiser 3 (ID3)\n",
    "# C4.5 and C5.0 (different versions of a powerful approach)\n",
    "# Chi-squared Automatic Interaction Detection (CHAID)\n",
    "# Decision Stump\n",
    "# M5\n",
    "# Conditional Decision Trees\n",
    "\n",
    "# Ensembling Algorithms:\n",
    "# Algorytmy które łączą wiele sląbszych algorytmów aby otrzymać dobry wynik:\n",
    "# Boosting\n",
    "# Bootstrapped Aggregation (Bagging)\n",
    "# AdaBoost\n",
    "# Weighted Average (Blending)\n",
    "# Stacked Generalization (Stacking)\n",
    "# Gradient Boosting Machines (GBM)\n",
    "# Gradient Boosted Regression Trees (GBRT)\n",
    "# Random Forest\n",
    "\n",
    "# Bayesian Algorithms:\n",
    "# Używają bayesian therom\n",
    "# Naive Bayes\n",
    "# Gaussian Naive Bayes\n",
    "# Multinomial Naive Bayes\n",
    "# Averaged One-Dependence Estimators (AODE)\n",
    "# Bayesian Belief Network (BBN)\n",
    "# Bayesian Network (BN)\n",
    "\n",
    "# Clustering Algorithms:\n",
    "# k-Means\n",
    "# k-Medians\n",
    "# Expectation Maximisation (EM)\n",
    "# Hierarchical Clustering\n",
    "\n",
    "# Association Rule Learning Algorithms:\n",
    "# Wyciągają informacje które najlepiej opisują powiązania podobieństwa obserwacjami.\n",
    "# Apriori algorithm\n",
    "# Eclat algorithm\n",
    "\n",
    "# Artificial Neural Network Algorithms:\n",
    "# Perceptron\n",
    "# Multilayer Perceptrons (MLP)\n",
    "# Back-Propagation\n",
    "# Stochastic Gradient Descent\n",
    "# Hopfield Network\n",
    "# Radial Basis Function Network (RBFN)\n",
    "\n",
    "# Deep Learning Algorithms\n",
    "# Convolutional Neural Network (CNN)\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "# Long Short-Term Memory Networks (LSTMs)\n",
    "# Stacked Auto-Encoders\n",
    "# Deep Boltzmann Machine (DBM)\n",
    "# Deep Belief Networks (DBN)\n",
    "\n",
    "# Dimensionality reduction:\n",
    "# Principal Component Analysis (PCA)\n",
    "# Principal Component Regression (PCR)\n",
    "# Partial Least Squares Regression (PLSR)\n",
    "# Sammon Mapping\n",
    "# Multidimensional Scaling (MDS)\n",
    "# Projection Pursuit\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "# Mixture Discriminant Analysis (MDA)\n",
    "# Quadratic Discriminant Analysis (QDA)\n",
    "# Flexible Discriminant Analysis (FDA)\n",
    "\n",
    "# Pozostałe Algorytmy:\n",
    "# Algorytmy do inncyh zdań jak:\n",
    "# Feature selection algorithms\n",
    "# Algorithm accuracy evaluation\n",
    "# Performance measures\n",
    "# Optimization algorithms\n",
    "# Algorytmy do subfields:\n",
    "# Computational intelligence (evolutionary algorithms, etc.)\n",
    "# Computer Vision (CV)\n",
    "# Natural Language Processing (NLP)\n",
    "# Recommender Systems\n",
    "# Reinforcement Learning\n",
    "# Graphical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cafad67-2219-4373-9664-e152e76fa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy datasets\n",
    "def load_dataset():\n",
    "    return make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "\n",
    "def load_dataset():\n",
    "    return make_regression(n_samples=1000, n_features=50, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec45626e-00d9-4e9a-92c9-476d0bee79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of models to test: {'ModelName': ModelObjectReference}\n",
    "def get_classification_models():\n",
    "    models=dict()\n",
    "    \n",
    "    # Linear base learning\n",
    "    models['LogisticRegression'] = LogisticRegression()\n",
    "    \n",
    "    # if model has hiperparameters, we will check many popular values for them\n",
    "    # to give models equal chances\n",
    "    alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        models[f'RidgeClassifier_({a})'] = RidgeClassifier(alpha=a)\n",
    "    \n",
    "    models['SGDClassifier'] = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    models['PassiveAggressiveClassifier'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "    \n",
    "    # Non Linear Learning\n",
    "    models['DecisionTreeClassifier'] = DecisionTreeClassifier()\n",
    "    models['ExtraTreeClassifier'] = ExtraTreeClassifier()\n",
    "    \n",
    "    for k in range(1, 21):\n",
    "        models[f'KNeighborsClassifier_({k})'] = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "    models['LinearSVM'] = SVC(kernel='linear')\n",
    "    models['PolynomialSVM'] = SVC(kernel='poly')\n",
    "    \n",
    "    c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for c in c_values:\n",
    "        models[f'SupportVectorClassifier_({c})'] = SVC(C=c)\n",
    "        \n",
    "    models['GaussianNaiveBayes'] = GaussianNB()\n",
    "    \n",
    "    # Ensemble Learning\n",
    "    n_trees = 100\n",
    "    models['AdaBoostClassifier'] = AdaBoostClassifier(n_estimators=n_trees)\n",
    "    models['BaggingClassifier'] = BaggingClassifier(n_estimators=n_trees)\n",
    "    models['RandomForestClassifier'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "    models['ExtraTreesClassifier'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "    models['GradientBoostingClassifier'] = GradientBoostingClassifier(n_estimators=n_trees)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b6fb54d-2115-48b8-8492-ada10df2a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_models():\n",
    "    models=dict()\n",
    "    \n",
    "    # Linear Base Learning\n",
    "    models['LinearRegression'] = LinearRegression()\n",
    "    alpha = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        models[f'LassoRegression_({a})'] = Lasso(alpha=a)\n",
    "    for a in alpha:\n",
    "        models[f'RidgeRegression_({a})'] = Ridge(alpha=a)\n",
    "    for l1, l2 in combinations(alpha, 2):\n",
    "        models[f'ElasticNetRegression_({l2})_({l1})'] = ElasticNet(alpha=l2, l1_ratio=l1)\n",
    "    \n",
    "    models['HuberRegression'] = HuberRegressor()\n",
    "    models['LarsRegression'] = Lars()\n",
    "    models['LassoLarsRegression'] = LassoLars()\n",
    "    models['PassiveAggressiveRegression'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['RANSACRegression'] = RANSACRegressor()\n",
    "    models['SGDRegression'] = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['TheilSenRegression'] = TheilSenRegressor()\n",
    "    \n",
    "    # Non Linear Learning\n",
    "    for k in range(1, 21):\n",
    "        models[f'KNeighborsRegressor_({k})'] = KNeighborsRegressor(n_neighbors=k)\n",
    "    models['TreeRegression'] = DecisionTreeRegressor()\n",
    "    models['ExtraTreeRegression'] = ExtraTreeRegressor()\n",
    "    models['LinearSupportVectorRegression'] = SVR(kernel='linear')\n",
    "    models['PolynomialSupportVectorRegression'] = SVR(kernel='poly')\n",
    "    c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for c in c_values:\n",
    "        models[f'SupportVectorRegression_({c})'] = SVR(C=c)\n",
    "    \n",
    "    # Ensemble Learning\n",
    "    n_trees = 100\n",
    "    models['AdaBoostRegressoion'] = AdaBoostRegressor(n_estimators=n_trees)\n",
    "    models['BaggingRegression'] = BaggingRegressor(n_estimators=n_trees)\n",
    "    models['RandomForestRegression'] = RandomForestRegressor(n_estimators=n_trees)\n",
    "    models['ExtraTreesRegression'] = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "    models['GradientBoostingRegression'] = GradientBoostingRegressor(n_estimators=n_trees)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8001d98c-316d-4ba7-952b-8ebb7fde8221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': LogisticRegression(),\n",
       " 'RidgeClassifier_(0.1)': RidgeClassifier(alpha=0.1),\n",
       " 'RidgeClassifier_(0.2)': RidgeClassifier(alpha=0.2),\n",
       " 'RidgeClassifier_(0.3)': RidgeClassifier(alpha=0.3),\n",
       " 'RidgeClassifier_(0.4)': RidgeClassifier(alpha=0.4),\n",
       " 'RidgeClassifier_(0.5)': RidgeClassifier(alpha=0.5),\n",
       " 'RidgeClassifier_(0.6)': RidgeClassifier(alpha=0.6),\n",
       " 'RidgeClassifier_(0.7)': RidgeClassifier(alpha=0.7),\n",
       " 'RidgeClassifier_(0.8)': RidgeClassifier(alpha=0.8),\n",
       " 'RidgeClassifier_(0.9)': RidgeClassifier(alpha=0.9),\n",
       " 'RidgeClassifier_(1.0)': RidgeClassifier(),\n",
       " 'SGDClassifier': SGDClassifier(),\n",
       " 'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),\n",
       " 'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
       " 'ExtraTreeClassifier': ExtraTreeClassifier(),\n",
       " 'KNeighborsClassifier_(1)': KNeighborsClassifier(n_neighbors=1),\n",
       " 'KNeighborsClassifier_(2)': KNeighborsClassifier(n_neighbors=2),\n",
       " 'KNeighborsClassifier_(3)': KNeighborsClassifier(n_neighbors=3),\n",
       " 'KNeighborsClassifier_(4)': KNeighborsClassifier(n_neighbors=4),\n",
       " 'KNeighborsClassifier_(5)': KNeighborsClassifier(),\n",
       " 'KNeighborsClassifier_(6)': KNeighborsClassifier(n_neighbors=6),\n",
       " 'KNeighborsClassifier_(7)': KNeighborsClassifier(n_neighbors=7),\n",
       " 'KNeighborsClassifier_(8)': KNeighborsClassifier(n_neighbors=8),\n",
       " 'KNeighborsClassifier_(9)': KNeighborsClassifier(n_neighbors=9),\n",
       " 'KNeighborsClassifier_(10)': KNeighborsClassifier(n_neighbors=10),\n",
       " 'KNeighborsClassifier_(11)': KNeighborsClassifier(n_neighbors=11),\n",
       " 'KNeighborsClassifier_(12)': KNeighborsClassifier(n_neighbors=12),\n",
       " 'KNeighborsClassifier_(13)': KNeighborsClassifier(n_neighbors=13),\n",
       " 'KNeighborsClassifier_(14)': KNeighborsClassifier(n_neighbors=14),\n",
       " 'KNeighborsClassifier_(15)': KNeighborsClassifier(n_neighbors=15),\n",
       " 'KNeighborsClassifier_(16)': KNeighborsClassifier(n_neighbors=16),\n",
       " 'KNeighborsClassifier_(17)': KNeighborsClassifier(n_neighbors=17),\n",
       " 'KNeighborsClassifier_(18)': KNeighborsClassifier(n_neighbors=18),\n",
       " 'KNeighborsClassifier_(19)': KNeighborsClassifier(n_neighbors=19),\n",
       " 'KNeighborsClassifier_(20)': KNeighborsClassifier(n_neighbors=20),\n",
       " 'LinearSVM': SVC(kernel='linear'),\n",
       " 'PolynomialSVM': SVC(kernel='poly'),\n",
       " 'SupportVectorClassifier_(0.1)': SVC(C=0.1),\n",
       " 'SupportVectorClassifier_(0.2)': SVC(C=0.2),\n",
       " 'SupportVectorClassifier_(0.3)': SVC(C=0.3),\n",
       " 'SupportVectorClassifier_(0.4)': SVC(C=0.4),\n",
       " 'SupportVectorClassifier_(0.5)': SVC(C=0.5),\n",
       " 'SupportVectorClassifier_(0.6)': SVC(C=0.6),\n",
       " 'SupportVectorClassifier_(0.7)': SVC(C=0.7),\n",
       " 'SupportVectorClassifier_(0.8)': SVC(C=0.8),\n",
       " 'SupportVectorClassifier_(0.9)': SVC(C=0.9),\n",
       " 'SupportVectorClassifier_(1.0)': SVC(),\n",
       " 'GaussianNaiveBayes': GaussianNB(),\n",
       " 'AdaBoostClassifier': AdaBoostClassifier(n_estimators=100),\n",
       " 'BaggingClassifier': BaggingClassifier(n_estimators=100),\n",
       " 'RandomForestClassifier': RandomForestClassifier(),\n",
       " 'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
       " 'GradientBoostingClassifier': GradientBoostingClassifier()}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = get_classification_models()\n",
    "# models = get_regression_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e8359-5c07-412a-9644-a99cda4bff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gradient boosting models\n",
    "def define_gbm_models(models=dict(), use_xgb=True):\n",
    "    # define config ranges\n",
    "    rates = [0.001, 0.01, 0.1]\n",
    "    trees = [50, 100]\n",
    "    ss = [0.5, 0.7, 1.0]\n",
    "    depth = [3, 7, 9]\n",
    "    # add configurations\n",
    "    for l in rates:\n",
    "        for e in trees:\n",
    "            for s in ss:\n",
    "                for d in depth:\n",
    "                    cfg = [l, e, s, d]\n",
    "                    if use_xgb:\n",
    "                        name = 'xgb-' + str(cfg)\n",
    "                        models[name] = XGBClassifier(learning_rate=l, n_estimators=e, subsample=s, max_depth=d)\n",
    "                    else:\n",
    "                        name = 'gbm-' + str(cfg)\n",
    "                        models[name] = GradientBoostingClassifier(learning_rate=l, n_estimators=e, subsample=s, max_depth=d)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    "\n",
    "# define gradient boosting models\n",
    "def get_gbm_models(models=dict(), use_xgb=True):\n",
    "    # define config ranges\n",
    "    rates = [0.001, 0.01, 0.1]\n",
    "    trees = [50, 100]\n",
    "    ss = [0.5, 0.7, 1.0]\n",
    "    depth = [3, 7, 9]\n",
    "    # add configurations\n",
    "    for l in rates:\n",
    "        for e in trees:\n",
    "            for s in ss:\n",
    "                for d in depth:\n",
    "                    cfg = [l, e, s, d]\n",
    "                    if use_xgb:\n",
    "                        name = 'xgb-' + str(cfg)\n",
    "                        models[name] = XGBRegressor(learning_rate=l, n_estimators=e, subsample=s, max_depth=d)\n",
    "                    else:\n",
    "                        name = 'gbm-' + str(cfg)\n",
    "                        models[name] = GradientBoostingXGBRegressor(learning_rate=l, n_estimators=e, subsample=s, max_depth=d)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    "\n",
    "\n",
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, metric):\n",
    "    # create the pipeline\n",
    "    pipeline = make_pipeline(model)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, repeats, metric):\n",
    "    # create the pipeline\n",
    "    pipeline = make_pipeline(model)\n",
    "    # evaluate model\n",
    "    scores = list()\n",
    "    # repeat model evaluation n times\n",
    "    for _ in range(repeats):\n",
    "        # perform run\n",
    "        scores_r = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "        # add scores to list\n",
    "        scores += scores_r.tolist()\n",
    "    return scores\n",
    "\n",
    "def robust_evaluate_model(X, y, model, folds, metric):\n",
    "    scores = None\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(X, y, model, folds, metric)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, folds=10, metric='accuracy'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        scores = robust_evaluate_model(X, y, model, folds, metric)\n",
    "        # show process\n",
    "        if scores is not None:\n",
    "            # store a result\n",
    "            results[name] = scores\n",
    "            mean_score, std_score = mean(scores), std(scores)\n",
    "            print('>%s: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "        else:\n",
    "            print('>%s:error' % name)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, pipe_funcs, folds=10, metric='accuracy'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate model under each preparation function\n",
    "        for i in range(len(pipe_funcs)):\n",
    "            # evaluate the model\n",
    "            scores = robust_evaluate_model(X, y, model, folds, metric, pipe_funcs[i])\n",
    "            # update name\n",
    "            run_name = str(i) + name\n",
    "            # show process\n",
    "            if scores is not None:\n",
    "                # store a result\n",
    "                results[run_name] = scores\n",
    "                mean_score, std_score = mean(scores), std(scores)\n",
    "                print('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "            else:\n",
    "                print('>%s: error' % run_name)\n",
    "    return results\n",
    "    # evaluate a model and try to trap errors and and hide warnings\n",
    "\n",
    "# no transforms pipeline\n",
    "def pipeline_none(model):\n",
    "    return model\n",
    " \n",
    "# standardize transform pipeline\n",
    "def pipeline_standardize(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    " \n",
    "# normalize transform pipeline\n",
    "def pipeline_normalize(model):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "# standardize and normalize pipeline\n",
    "def pipeline_std_norm(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "pipelines = [pipeline_none, pipeline_standardize, pipeline_normalize, pipeline_std_norm]\n",
    "\n",
    "\n",
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "    # check for no results\n",
    "    if len(results) == 0:\n",
    "        print('no results')\n",
    "        return\n",
    "    # determine how many results to summarize\n",
    "    n = min(top_n, len(results))\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    # retrieve the top n for summarization\n",
    "    names = [x[0] for x in mean_scores[:n]]\n",
    "    scores = [results[x[0]] for x in mean_scores[:n]]\n",
    "    # print the top n\n",
    "    print()\n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        mean_score, std_score = mean(results[name]), std(results[name])\n",
    "        print('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "    # boxplot for the top n\n",
    "    pyplot.boxplot(scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    pyplot.savefig('spotcheck.png')\n",
    "\n",
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e9d07-4aa2-4aa7-a291-b65d389ed617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression spot check script\n",
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    " \n",
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "\treturn make_regression(n_samples=1000, n_features=50, noise=0.1, random_state=1)\n",
    " \n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def get_models(models=dict()):\n",
    "\t# linear models\n",
    "\tmodels['lr'] = LinearRegression()\n",
    "\talpha = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\tfor a in alpha:\n",
    "\t\tmodels['lasso-'+str(a)] = Lasso(alpha=a)\n",
    "\tfor a in alpha:\n",
    "\t\tmodels['ridge-'+str(a)] = Ridge(alpha=a)\n",
    "\tfor a1 in alpha:\n",
    "\t\tfor a2 in alpha:\n",
    "\t\t\tname = 'en-' + str(a1) + '-' + str(a2)\n",
    "\t\t\tmodels[name] = ElasticNet(a1, a2)\n",
    "\tmodels['huber'] = HuberRegressor()\n",
    "\tmodels['lars'] = Lars()\n",
    "\tmodels['llars'] = LassoLars()\n",
    "\tmodels['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n",
    "\tmodels['ranscac'] = RANSACRegressor()\n",
    "\tmodels['sgd'] = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\tmodels['theil'] = TheilSenRegressor()\n",
    "\t# non-linear models\n",
    "\tn_neighbors = range(1, 21)\n",
    "\tfor k in n_neighbors:\n",
    "\t\tmodels['knn-'+str(k)] = KNeighborsRegressor(n_neighbors=k)\n",
    "\tmodels['cart'] = DecisionTreeRegressor()\n",
    "\tmodels['extra'] = ExtraTreeRegressor()\n",
    "\tmodels['svml'] = SVR(kernel='linear')\n",
    "\tmodels['svmp'] = SVR(kernel='poly')\n",
    "\tc_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\tfor c in c_values:\n",
    "\t\tmodels['svmr'+str(c)] = SVR(C=c)\n",
    "\t# ensemble models\n",
    "\tn_trees = 100\n",
    "\tmodels['ada'] = AdaBoostRegressor(n_estimators=n_trees)\n",
    "\tmodels['bag'] = BaggingRegressor(n_estimators=n_trees)\n",
    "\tmodels['rf'] = RandomForestRegressor(n_estimators=n_trees)\n",
    "\tmodels['et'] = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "\tmodels['gbm'] = GradientBoostingRegressor(n_estimators=n_trees)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    " \n",
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, metric):\n",
    "\t# create the pipeline\n",
    "\tpipeline = make_pipeline(model)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "\treturn scores\n",
    " \n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, folds, metric):\n",
    "\tscores = None\n",
    "\ttry:\n",
    "\t\twith warnings.catch_warnings():\n",
    "\t\t\twarnings.filterwarnings(\"ignore\")\n",
    "\t\t\tscores = evaluate_model(X, y, model, folds, metric)\n",
    "\texcept:\n",
    "\t\tscores = None\n",
    "\treturn scores\n",
    " \n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, folds=10, metric='accuracy'):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tscores = robust_evaluate_model(X, y, model, folds, metric)\n",
    "\t\t# show process\n",
    "\t\tif scores is not None:\n",
    "\t\t\t# store a result\n",
    "\t\t\tresults[name] = scores\n",
    "\t\t\tmean_score, std_score = mean(scores), std(scores)\n",
    "\t\t\tprint('>%s: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "\t\telse:\n",
    "\t\t\tprint('>%s: error' % name)\n",
    "\treturn results\n",
    " \n",
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = mean(results[name]), std(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')\n",
    " \n",
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = get_models()\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models, metric='neg_mean_squared_error')\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
